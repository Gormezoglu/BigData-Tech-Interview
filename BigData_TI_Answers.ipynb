{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ybl-OTpuiQx"
      },
      "source": [
        "## Task -1 Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2FGpclC3tn4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Installing Spark with its dependencies\n",
        "Installing Spark\n",
        "Install Dependencies:\n",
        "\n",
        "Java 8\n",
        "Apache Spark with hadoop and\n",
        "Findspark (used to locate the spark in the system)\n",
        "\"\"\"\n",
        "\n",
        "!sudo ./install_spark.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1vd9yVaK4DR6"
      },
      "outputs": [],
      "source": [
        "#Set Environment Variables:\n",
        "\n",
        "import os\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = os.path.join(current_directory,\"spark-3.1.1-bin-hadoop3.2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "SUjv6TWh4i1t",
        "outputId": "9bb2d972-e87e-43fb-d2f5-fca2dd7c7b1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/01/23 12:17:50 WARN Utils: Your hostname, codespaces-7047d6 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
            "24/01/23 12:17:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "24/01/23 12:17:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6f4b8caf-0ea5-4eb4-919f-2c705848fca2.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MovieLens</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fa967357250>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#Create a SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"MovieLens\") \\\n",
        "    .config(\"spark.jars\", \"spark-3.1.1-bin-hadoop3.2/jars/sqlite-jdbc-3.34.0.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2mYOKttpn9EH"
      },
      "outputs": [],
      "source": [
        "#will be used sqlite3 to be able to reach .db file\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "con = sqlite3.connect('Datasets/movielens-small.db')\n",
        "cur = con.cursor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Table: movies\n",
            "0: movieId: INT\n",
            "1: title: TEXT\n",
            "2: year: INT\n",
            "3: genres: TEXT\n",
            "\n",
            "Table: ratings\n",
            "0: userId: INT\n",
            "1: movieId: INT\n",
            "2: rating: REAL\n",
            "3: timestamp: INT\n",
            "\n",
            "Table: links\n",
            "0: movieId: INT\n",
            "1: imdbId: TEXT\n",
            "2: tmdbId: TEXT\n",
            "\n",
            "Table: tags\n",
            "0: userId: INT\n",
            "1: movieId: INT\n",
            "2: tag: TEXT\n",
            "3: timestamp: NUM\n"
          ]
        }
      ],
      "source": [
        "# Get the field information of each table in the database\n",
        "\n",
        "# Define a function to print table information in a readable way\n",
        "def print_table_info(table_name, table_info):\n",
        "    print(f\"\\nTable: {table_name}\")\n",
        "    for row in table_info:\n",
        "        print(row[0], row[1], row[2],sep=\": \")\n",
        "\n",
        "# Get and print information for the 'movies' table\n",
        "cur.execute(\"PRAGMA table_info(movies);\")\n",
        "movies_info = cur.fetchall()\n",
        "print_table_info(\"movies\", movies_info)\n",
        "\n",
        "# Get and print information for the 'ratings' table\n",
        "cur.execute(\"PRAGMA table_info(ratings);\")\n",
        "ratings_info = cur.fetchall()\n",
        "print_table_info(\"ratings\", ratings_info)\n",
        "\n",
        "# Get and print information for the 'links' table\n",
        "cur.execute(\"PRAGMA table_info(links);\")\n",
        "links_info = cur.fetchall()\n",
        "print_table_info(\"links\", links_info)\n",
        "\n",
        "# Get and print information for the 'tags' table\n",
        "cur.execute(\"PRAGMA table_info(tags);\")\n",
        "tags_info = cur.fetchall()\n",
        "print_table_info(\"tags\", tags_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OahMiKvLqz6p",
        "outputId": "9b7a9f5d-808f-409d-e6fa-8b5d122c5de0"
      },
      "outputs": [],
      "source": [
        "#For SQLite JDBC driver, it can be downloaded via:\n",
        "\n",
        "!curl -O https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HikariCP-2.5.1.jar\n",
            "JLargeArrays-1.5.jar\n",
            "JTransforms-3.1.jar\n",
            "RoaringBitmap-0.9.0.jar\n",
            "ST4-4.0.4.jar\n",
            "accessors-smart-1.2.jar\n",
            "activation-1.1.1.jar\n",
            "aircompressor-0.10.jar\n",
            "algebra_2.12-2.0.0-M2.jar\n",
            "antlr-runtime-3.5.2.jar\n",
            "antlr4-runtime-4.8-1.jar\n",
            "aopalliance-1.0.jar\n",
            "aopalliance-repackaged-2.6.1.jar\n",
            "arpack_combined_all-0.1.jar\n",
            "arrow-format-2.0.0.jar\n",
            "arrow-memory-core-2.0.0.jar\n",
            "arrow-memory-netty-2.0.0.jar\n",
            "arrow-vector-2.0.0.jar\n",
            "audience-annotations-0.5.0.jar\n",
            "automaton-1.11-8.jar\n",
            "avro-1.8.2.jar\n",
            "avro-ipc-1.8.2.jar\n",
            "avro-mapred-1.8.2-hadoop2.jar\n",
            "bonecp-0.8.0.RELEASE.jar\n",
            "breeze-macros_2.12-1.0.jar\n",
            "breeze_2.12-1.0.jar\n",
            "cats-kernel_2.12-2.0.0-M4.jar\n",
            "chill-java-0.9.5.jar\n",
            "chill_2.12-0.9.5.jar\n",
            "commons-beanutils-1.9.4.jar\n",
            "commons-cli-1.2.jar\n",
            "commons-codec-1.10.jar\n",
            "commons-collections-3.2.2.jar\n",
            "commons-compiler-3.0.16.jar\n",
            "commons-compress-1.20.jar\n",
            "commons-configuration2-2.1.1.jar\n",
            "commons-crypto-1.1.0.jar\n",
            "commons-daemon-1.0.13.jar\n",
            "commons-dbcp-1.4.jar\n",
            "commons-httpclient-3.1.jar\n",
            "commons-io-2.5.jar\n",
            "commons-lang-2.6.jar\n",
            "commons-lang3-3.10.jar\n",
            "commons-logging-1.1.3.jar\n",
            "commons-math3-3.4.1.jar\n",
            "commons-net-3.1.jar\n",
            "commons-pool-1.5.4.jar\n",
            "commons-text-1.6.jar\n",
            "compress-lzf-1.0.3.jar\n",
            "core-1.1.2.jar\n",
            "curator-client-2.13.0.jar\n",
            "curator-framework-2.13.0.jar\n",
            "curator-recipes-2.13.0.jar\n",
            "datanucleus-api-jdo-4.2.4.jar\n",
            "datanucleus-core-4.1.17.jar\n",
            "datanucleus-rdbms-4.1.19.jar\n",
            "derby-10.12.1.1.jar\n",
            "dnsjava-2.1.7.jar\n",
            "dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "ehcache-3.3.1.jar\n",
            "flatbuffers-java-1.9.0.jar\n",
            "generex-1.0.2.jar\n",
            "geronimo-jcache_1.0_spec-1.0-alpha-1.jar\n",
            "gson-2.2.4.jar\n",
            "guava-14.0.1.jar\n",
            "guice-4.0.jar\n",
            "guice-servlet-4.0.jar\n",
            "hadoop-annotations-3.2.0.jar\n",
            "hadoop-auth-3.2.0.jar\n",
            "hadoop-client-3.2.0.jar\n",
            "hadoop-common-3.2.0.jar\n",
            "hadoop-hdfs-client-3.2.0.jar\n",
            "hadoop-mapreduce-client-common-3.2.0.jar\n",
            "hadoop-mapreduce-client-core-3.2.0.jar\n",
            "hadoop-mapreduce-client-jobclient-3.2.0.jar\n",
            "hadoop-yarn-api-3.2.0.jar\n",
            "hadoop-yarn-client-3.2.0.jar\n",
            "hadoop-yarn-common-3.2.0.jar\n",
            "hadoop-yarn-registry-3.2.0.jar\n",
            "hadoop-yarn-server-common-3.2.0.jar\n",
            "hadoop-yarn-server-web-proxy-3.2.0.jar\n",
            "hive-beeline-2.3.7.jar\n",
            "hive-cli-2.3.7.jar\n",
            "hive-common-2.3.7.jar\n",
            "hive-exec-2.3.7-core.jar\n",
            "hive-jdbc-2.3.7.jar\n",
            "hive-llap-common-2.3.7.jar\n",
            "hive-metastore-2.3.7.jar\n",
            "hive-serde-2.3.7.jar\n",
            "hive-service-rpc-3.1.2.jar\n",
            "hive-shims-0.23-2.3.7.jar\n",
            "hive-shims-2.3.7.jar\n",
            "hive-shims-common-2.3.7.jar\n",
            "hive-shims-scheduler-2.3.7.jar\n",
            "hive-storage-api-2.7.2.jar\n",
            "hive-vector-code-gen-2.3.7.jar\n",
            "hk2-api-2.6.1.jar\n",
            "hk2-locator-2.6.1.jar\n",
            "hk2-utils-2.6.1.jar\n",
            "htrace-core4-4.1.0-incubating.jar\n",
            "httpclient-4.5.6.jar\n",
            "httpcore-4.4.12.jar\n",
            "istack-commons-runtime-3.0.8.jar\n",
            "ivy-2.4.0.jar\n",
            "jackson-annotations-2.10.0.jar\n",
            "jackson-core-2.10.0.jar\n",
            "jackson-core-asl-1.9.13.jar\n",
            "jackson-databind-2.10.0.jar\n",
            "jackson-dataformat-yaml-2.10.0.jar\n",
            "jackson-datatype-jsr310-2.11.2.jar\n",
            "jackson-jaxrs-base-2.9.5.jar\n",
            "jackson-jaxrs-json-provider-2.9.5.jar\n",
            "jackson-mapper-asl-1.9.13.jar\n",
            "jackson-module-jaxb-annotations-2.10.0.jar\n",
            "jackson-module-paranamer-2.10.0.jar\n",
            "jackson-module-scala_2.12-2.10.0.jar\n",
            "jakarta.activation-api-1.2.1.jar\n",
            "jakarta.annotation-api-1.3.5.jar\n",
            "jakarta.inject-2.6.1.jar\n",
            "jakarta.servlet-api-4.0.3.jar\n",
            "jakarta.validation-api-2.0.2.jar\n",
            "jakarta.ws.rs-api-2.1.6.jar\n",
            "jakarta.xml.bind-api-2.3.2.jar\n",
            "janino-3.0.16.jar\n",
            "javassist-3.25.0-GA.jar\n",
            "javax.inject-1.jar\n",
            "javax.jdo-3.2.0-m3.jar\n",
            "javolution-5.5.1.jar\n",
            "jaxb-api-2.2.11.jar\n",
            "jaxb-runtime-2.3.2.jar\n",
            "jcip-annotations-1.0-1.jar\n",
            "jcl-over-slf4j-1.7.30.jar\n",
            "jdo-api-3.0.1.jar\n",
            "jersey-client-2.30.jar\n",
            "jersey-common-2.30.jar\n",
            "jersey-container-servlet-2.30.jar\n",
            "jersey-container-servlet-core-2.30.jar\n",
            "jersey-hk2-2.30.jar\n",
            "jersey-media-jaxb-2.30.jar\n",
            "jersey-server-2.30.jar\n",
            "jline-2.14.6.jar\n",
            "joda-time-2.10.5.jar\n",
            "jodd-core-3.5.2.jar\n",
            "jpam-1.1.jar\n",
            "json-1.8.jar\n",
            "json-smart-2.3.jar\n",
            "json4s-ast_2.12-3.7.0-M5.jar\n",
            "json4s-core_2.12-3.7.0-M5.jar\n",
            "json4s-jackson_2.12-3.7.0-M5.jar\n",
            "json4s-scalap_2.12-3.7.0-M5.jar\n",
            "jsp-api-2.1.jar\n",
            "jsr305-3.0.0.jar\n",
            "jta-1.1.jar\n",
            "jul-to-slf4j-1.7.30.jar\n",
            "kerb-admin-1.0.1.jar\n",
            "kerb-client-1.0.1.jar\n",
            "kerb-common-1.0.1.jar\n",
            "kerb-core-1.0.1.jar\n",
            "kerb-crypto-1.0.1.jar\n",
            "kerb-identity-1.0.1.jar\n",
            "kerb-server-1.0.1.jar\n",
            "kerb-simplekdc-1.0.1.jar\n",
            "kerb-util-1.0.1.jar\n",
            "kerby-asn1-1.0.1.jar\n",
            "kerby-config-1.0.1.jar\n",
            "kerby-pkix-1.0.1.jar\n",
            "kerby-util-1.0.1.jar\n",
            "kerby-xdr-1.0.1.jar\n",
            "kryo-shaded-4.0.2.jar\n",
            "kubernetes-client-4.12.0.jar\n",
            "kubernetes-model-admissionregistration-4.12.0.jar\n",
            "kubernetes-model-apiextensions-4.12.0.jar\n",
            "kubernetes-model-apps-4.12.0.jar\n",
            "kubernetes-model-autoscaling-4.12.0.jar\n",
            "kubernetes-model-batch-4.12.0.jar\n",
            "kubernetes-model-certificates-4.12.0.jar\n",
            "kubernetes-model-common-4.12.0.jar\n",
            "kubernetes-model-coordination-4.12.0.jar\n",
            "kubernetes-model-core-4.12.0.jar\n",
            "kubernetes-model-discovery-4.12.0.jar\n",
            "kubernetes-model-events-4.12.0.jar\n",
            "kubernetes-model-extensions-4.12.0.jar\n",
            "kubernetes-model-metrics-4.12.0.jar\n",
            "kubernetes-model-networking-4.12.0.jar\n",
            "kubernetes-model-policy-4.12.0.jar\n",
            "kubernetes-model-rbac-4.12.0.jar\n",
            "kubernetes-model-scheduling-4.12.0.jar\n",
            "kubernetes-model-settings-4.12.0.jar\n",
            "kubernetes-model-storageclass-4.12.0.jar\n",
            "leveldbjni-all-1.8.jar\n",
            "libfb303-0.9.3.jar\n",
            "libthrift-0.12.0.jar\n",
            "log4j-1.2.17.jar\n",
            "logging-interceptor-3.12.12.jar\n",
            "lz4-java-1.7.1.jar\n",
            "machinist_2.12-0.6.8.jar\n",
            "macro-compat_2.12-1.1.1.jar\n",
            "mesos-1.4.0-shaded-protobuf.jar\n",
            "metrics-core-4.1.1.jar\n",
            "metrics-graphite-4.1.1.jar\n",
            "metrics-jmx-4.1.1.jar\n",
            "metrics-json-4.1.1.jar\n",
            "metrics-jvm-4.1.1.jar\n",
            "minlog-1.3.0.jar\n",
            "netty-all-4.1.51.Final.jar\n",
            "nimbus-jose-jwt-4.41.1.jar\n",
            "objenesis-2.6.jar\n",
            "okhttp-2.7.5.jar\n",
            "okhttp-3.12.12.jar\n",
            "okio-1.14.0.jar\n",
            "opencsv-2.3.jar\n",
            "orc-core-1.5.12.jar\n",
            "orc-mapreduce-1.5.12.jar\n",
            "orc-shims-1.5.12.jar\n",
            "oro-2.0.8.jar\n",
            "osgi-resource-locator-1.0.3.jar\n",
            "paranamer-2.8.jar\n",
            "parquet-column-1.10.1.jar\n",
            "parquet-common-1.10.1.jar\n",
            "parquet-encoding-1.10.1.jar\n",
            "parquet-format-2.4.0.jar\n",
            "parquet-hadoop-1.10.1.jar\n",
            "parquet-jackson-1.10.1.jar\n",
            "protobuf-java-2.5.0.jar\n",
            "py4j-0.10.9.jar\n",
            "pyrolite-4.30.jar\n",
            "re2j-1.1.jar\n",
            "scala-collection-compat_2.12-2.1.1.jar\n",
            "scala-compiler-2.12.10.jar\n",
            "scala-library-2.12.10.jar\n",
            "scala-parser-combinators_2.12-1.1.2.jar\n",
            "scala-reflect-2.12.10.jar\n",
            "scala-xml_2.12-1.2.0.jar\n",
            "shapeless_2.12-2.3.3.jar\n",
            "shims-0.9.0.jar\n",
            "slf4j-api-1.7.30.jar\n",
            "slf4j-log4j12-1.7.30.jar\n",
            "snakeyaml-1.24.jar\n",
            "snappy-java-1.1.8.2.jar\n",
            "spark-catalyst_2.12-3.1.1.jar\n",
            "spark-core_2.12-3.1.1.jar\n",
            "spark-graphx_2.12-3.1.1.jar\n",
            "spark-hive-thriftserver_2.12-3.1.1.jar\n",
            "spark-hive_2.12-3.1.1.jar\n",
            "spark-kubernetes_2.12-3.1.1.jar\n",
            "spark-kvstore_2.12-3.1.1.jar\n",
            "spark-launcher_2.12-3.1.1.jar\n",
            "spark-mesos_2.12-3.1.1.jar\n",
            "spark-mllib-local_2.12-3.1.1.jar\n",
            "spark-mllib_2.12-3.1.1.jar\n",
            "spark-network-common_2.12-3.1.1.jar\n",
            "spark-network-shuffle_2.12-3.1.1.jar\n",
            "spark-repl_2.12-3.1.1.jar\n",
            "spark-sketch_2.12-3.1.1.jar\n",
            "spark-sql_2.12-3.1.1.jar\n",
            "spark-streaming_2.12-3.1.1.jar\n",
            "spark-tags_2.12-3.1.1-tests.jar\n",
            "spark-tags_2.12-3.1.1.jar\n",
            "spark-unsafe_2.12-3.1.1.jar\n",
            "spark-yarn_2.12-3.1.1.jar\n",
            "spire-macros_2.12-0.17.0-M1.jar\n",
            "spire-platform_2.12-0.17.0-M1.jar\n",
            "spire-util_2.12-0.17.0-M1.jar\n",
            "spire_2.12-0.17.0-M1.jar\n",
            "sqlite-jdbc-3.34.0.jar\n",
            "stax-api-1.0.1.jar\n",
            "stax2-api-3.1.4.jar\n",
            "stream-2.9.6.jar\n",
            "super-csv-2.2.0.jar\n",
            "threeten-extra-1.5.0.jar\n",
            "token-provider-1.0.1.jar\n",
            "transaction-api-1.1.jar\n",
            "univocity-parsers-2.9.1.jar\n",
            "velocity-1.5.jar\n",
            "woodstox-core-5.0.3.jar\n",
            "xbean-asm7-shaded-4.15.jar\n",
            "xz-1.5.jar\n",
            "zjsonpatch-0.3.0.jar\n",
            "zookeeper-3.4.14.jar\n",
            "zstd-jni-1.4.8-1.jar\n"
          ]
        }
      ],
      "source": [
        "# install sqlite-jdbc-3.34.0.jar to the spark/jars directory\n",
        "\n",
        "!sudo cp sqlite-jdbc-3.34.0.jar spark-3.1.1-bin-hadoop3.2/jars/\n",
        "\n",
        "#check the jar file is in the spark/jars directory\n",
        "\n",
        "!ls spark-3.1.1-bin-hadoop3.2/jars/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a dataframe with including userid, movieid, genre and rating via pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Read the ratings table into a dataframe\n",
        "ratings_df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:sqlite:Datasets/movielens-small.db\") \\\n",
        "    .option(\"dbtable\", \"ratings\") \\\n",
        "    .load()\n",
        "\n",
        "# Read the movies table into a dataframe\n",
        "movies_df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:sqlite:Datasets/movielens-small.db\") \\\n",
        "    .option(\"dbtable\", \"movies\") \\\n",
        "    .load()\n",
        "\n",
        "# Read the links table into a dataframe\n",
        "links_df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:sqlite:Datasets/movielens-small.db\") \\\n",
        "    .option(\"dbtable\", \"links\") \\\n",
        "    .load()\n",
        "\n",
        "# Read the tags table into a dataframe\n",
        "tags_df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:sqlite:Datasets/movielens-small.db\") \\\n",
        "    .option(\"dbtable\", \"tags\") \\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+------+------+\n",
            "|userId|movieId|genres|rating|\n",
            "+------+-------+------+------+\n",
            "|    14|     26| Drama|   4.0|\n",
            "|    31|     26| Drama|   2.0|\n",
            "|    51|     26| Drama|   4.0|\n",
            "|    79|     26| Drama|   4.0|\n",
            "|   156|     26| Drama|   4.0|\n",
            "|   161|     26| Drama|   3.0|\n",
            "|   203|     26| Drama|   4.0|\n",
            "|   219|     26| Drama|   3.0|\n",
            "|   220|     26| Drama|   2.5|\n",
            "|   228|     26| Drama|   4.0|\n",
            "+------+-------+------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "100023"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write a query to create a dataframe with including userid, movieid, genre and rating\n",
        "\n",
        "\n",
        "# join ratings and movies dataframes on movieid column and create a new dataframe called ratings_movies_df via pyspark dataframe\n",
        "ratings_movies_df = ratings_df.join(movies_df, on=\"movieId\", how=\"left\")\n",
        "\n",
        "# show the userid, movieid, genre and rating columns of ratings_movies_df dataframe\n",
        "ratings_movies_df.select(\"userId\", \"movieId\", \"genres\", \"rating\").show(10)\n",
        "\n",
        "#count the number of rows in ratings_movies_df dataframe\n",
        "ratings_movies_df.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9adqNcucRA00",
        "outputId": "c08790f9-8ab1-4de5-8c3f-958c4d1c7365"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 291:============================================>        (167 + 2) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------------------+------------+\n",
            "|movieId|title                    |rating_count|\n",
            "+-------+-------------------------+------------+\n",
            "|593    |Silence of the Lambs, The|337         |\n",
            "|318    |Shawshank Redemption, The|328         |\n",
            "|296    |Pulp Fiction             |327         |\n",
            "|480    |Jurassic Park            |324         |\n",
            "|356    |Forrest Gump             |318         |\n",
            "+-------+-------------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Count ratings for each movie, and list top 5 movies with the highest value\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Calculate the count of ratings for each movie\n",
        "movie_rating_count_df = ratings_movies_df.groupBy(\"movieId\").agg(\n",
        "    F.count(\"rating\").alias(\"rating_count\")\n",
        ")\n",
        "\n",
        "# Join the top_movies_df with the movies_df dataframe to get the movie titles\n",
        "top_movies_df = movie_rating_count_df.join(movies_df, on=\"movieId\", how=\"left\")\n",
        "\n",
        "top_movies_df.select(\"movieId\", \"title\", \"rating_count\").sort(\"rating_count\", ascending=False).show(5, truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgVd_W6qkFC_",
        "outputId": "7dccc28f-4a43-4244-dc05-5c1aacdb347d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 282:===================================================> (196 + 2) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------------+\n",
            "|genres        |rating_count|\n",
            "+--------------+------------+\n",
            "|Drama         |7008        |\n",
            "|Comedy        |6396        |\n",
            "|Comedy|Romance|3877        |\n",
            "|Drama|Romance |3121        |\n",
            "|Comedy|Drama  |3000        |\n",
            "+--------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Find and list top 5 most rated genres\n",
        "\n",
        "genre_rating_count_df = ratings_movies_df.groupBy(\"genres\").agg(\n",
        "    F.count(\"rating\").alias(\"rating_count\")\n",
        ")\n",
        "\n",
        "genre_rating_count_df.sort(\"rating_count\", ascending=False).show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4OzuDxlcDsL",
        "outputId": "ef97a8b8-5517-495c-a035-56c6c1562f5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 300:==================================================>  (191 + 2) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+\n",
            "|tag         |rating_count|\n",
            "+------------+------------+\n",
            "|drama       |3542        |\n",
            "|sci-fi      |3035        |\n",
            "|twist ending|2998        |\n",
            "|psychology  |2672        |\n",
            "|crime       |2570        |\n",
            "+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#Find and list top 5 most rated tags except null values\n",
        "\n",
        "tags_rating_count_df = ratings_movies_df.join(tags_df, on=\"movieId\", how=\"left\").groupBy(\"tag\").agg(\n",
        "    F.count(\"rating\").alias(\"rating_count\")).filter(tags_df.tag.isNotNull())\n",
        "\n",
        "tags_rating_count_df.sort(\"rating_count\", ascending=False).show(5, truncate=False)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zWDyIsOk-uz",
        "outputId": "d4ad6c27-10c1-4f19-ad5e-e837a26c8012"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 313:==================================>                  (132 + 2) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------------+\n",
            "|userId|timestamp_count|\n",
            "+------+---------------+\n",
            "|516   |2268           |\n",
            "|384   |1412           |\n",
            "|187   |1338           |\n",
            "|31    |1283           |\n",
            "|377   |1241           |\n",
            "+------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# By using timestamp from ratings table, provide top 5 most frequent users within a week\n",
        "\n",
        "genre_rating_count_df = ratings_movies_df.groupBy(\"userId\").agg(\n",
        "    F.count(\"timestamp\").alias(\"timestamp_count\")\n",
        ")\n",
        "\n",
        "genre_rating_count_df.sort(\"timestamp_count\", ascending=False).show(5, truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK3PElZAxM7a",
        "outputId": "30ae116e-0681-41a4-84d1-f1d14be82561"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 317:====================================================>(199 + 1) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------+----------+\n",
            "|genres                                                  |rating_avg|\n",
            "+--------------------------------------------------------+----------+\n",
            "|Animation|Documentary                                   |5.0       |\n",
            "|Crime|Documentary|War                                   |5.0       |\n",
            "|Adventure|Fantasy|Mystery                               |5.0       |\n",
            "|Action|Adventure|Animation|Comedy|Fantasy|Mystery|Sci-Fi|5.0       |\n",
            "|Crime|Horror|Mystery                                    |4.75      |\n",
            "|Animation|Comedy|Horror|IMAX                            |4.5       |\n",
            "|Comedy|Crime|Western                                    |4.5       |\n",
            "|Adventure|Crime|Drama|Horror|Mystery                    |4.5       |\n",
            "|Adventure|Comedy|Crime|Drama|Romance                    |4.5       |\n",
            "|Action|Animation|Drama|Fantasy|Sci-Fi                   |4.5       |\n",
            "+--------------------------------------------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Calculate average ratings for each genre, and plot average ratings of top 10 genres with descending order\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Calculate the average rating for each genre\n",
        "genre_rating_avg_df = ratings_movies_df.groupBy(\"genres\").agg(\n",
        "    F.avg(\"rating\").alias(\"rating_avg\")\n",
        ")\n",
        "\n",
        "# Sort the dataframe by rating_avg column\n",
        "genre_rating_avg_df.sort(\"rating_avg\", ascending=False).show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M2z4B1ZubTP"
      },
      "source": [
        "## TASK 2 - Recommender Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd6rfJRvuf0U"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0YoCcJzPkrB"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"movieRecommendation\").getOrCreate() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "XlqTJcjFSdHr",
        "outputId": "4f9bf3d2-7be9-42f8-af34-eb03fc282028"
      },
      "outputs": [],
      "source": [
        "# using movie rating data to probide implicit feature using ALS(Alternate Least Squares)\n",
        "\n",
        "movie_rating=[]\n",
        "\n",
        "for row in cur.execute(\"\"\"SELECT userId, movieId, rating \n",
        "                          FROM ratings\n",
        "                                      \"\"\"):\n",
        "  movie_rating.append(row)\n",
        "\n",
        "schema = [\"userId\", \"movieId\", \"rating\"]\n",
        " \n",
        "# calling function to create dataframe\n",
        "movie_rating_df = spark.createDataFrame(movie_rating, schema)\n",
        "\n",
        "movie_rating_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sHsJtxjKL9A",
        "outputId": "0d4470c6-577b-47d7-94be-fb36f80937d6"
      },
      "outputs": [],
      "source": [
        "# description of created schema \n",
        "movie_rating_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0xQG5w9ryG8"
      },
      "outputs": [],
      "source": [
        "#splitting dataset to train the model as 80% for train and remaining for test data.\n",
        "(train, test) = movie_rating_df.randomSplit([0.8, 0.2], seed=87)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGTNsGakWNl6",
        "outputId": "09fa8371-5c7b-425b-de3b-0cb88923645b"
      },
      "outputs": [],
      "source": [
        "# 1st Recommender model - Alternating Least Square (ALS) Matrix Factorization in Collaborative Filtering on rating (as actual values) \n",
        "\n",
        "als = ALS(rank=10, maxIter=15, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
        "\n",
        "model = als.fit(train)\n",
        "\n",
        "pred = model.transform(test)\n",
        "\n",
        "pred = pred.selectExpr(\"userId as userId\",\"movieId as movieId\",\"rating as rating\",\"prediction as implicit\")\n",
        "\n",
        "pred.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IxWorgqcUIO",
        "outputId": "bf56b94f-7c1b-4cb9-fc51-04d2eb244d95"
      },
      "outputs": [],
      "source": [
        "#calculating RMSE and MAE to evaluate performance of the models. \n",
        "\n",
        "eval_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"implicit\")\n",
        "eval_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"implicit\")\n",
        "\n",
        "\n",
        "rmse = eval_rmse.evaluate(pred)\n",
        "mae = eval_mae.evaluate(pred)\n",
        "\n",
        "\n",
        "print(\"RMSE of ALS:\", rmse)\n",
        "print(\"MAE of ALS:\", mae)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiveQdC4vbT-",
        "outputId": "792174a8-f360-48ab-9e95-b73b6c8f1410"
      },
      "outputs": [],
      "source": [
        "# 2nd Recommender model - Alternating Least Square (ALS) Matrix Factorization in Collaborative Filtering on designed implicit feedback values \n",
        "\n",
        "\n",
        "(train_implicit, test_implicit) = pred.randomSplit([0.8, 0.2], seed=87)\n",
        "\n",
        "als_implicit = ALS(rank=10, maxIter=15, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"implicit\", coldStartStrategy=\"drop\")\n",
        "\n",
        "model_implicit = als_implicit.fit(train_implicit)\n",
        "\n",
        "pred_implicit = model_implicit.transform(test_implicit)\n",
        "\n",
        "pred_implicit.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oUlsznN7MGh",
        "outputId": "b1757443-91a5-44e8-8f03-32982684794f"
      },
      "outputs": [],
      "source": [
        "eval_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "eval_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "\n",
        "\n",
        "rmse = eval_rmse.evaluate(pred_implicit)\n",
        "mae = eval_mae.evaluate(pred_implicit)\n",
        "\n",
        "\n",
        "print(\"RMSE of ALS_Implicit Feedback:\", rmse)\n",
        "print(\"MAE of ALS_Implicit Feedback:\", mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imQMCTu799AA"
      },
      "source": [
        "When compared these two models, the 1st model ( ALS on rating ) shows better performance compared to 2nd model (ALS on implicit feedback) according to error metrics such as Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeRDX3e3-4z5"
      },
      "source": [
        "## Task â€“ 3 Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY-LEz8E-8CP"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"textAnalysis\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXVe2XVGHOut"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download the data in Dataset folder.\n",
        "\n",
        "\n",
        "\n",
        "!curl -o Datasets/aclImdb_v1.tar.gz https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_13BV8pDCDiN",
        "outputId": "d7ac9527-f4af-4524-824a-f5a679f38650"
      },
      "outputs": [],
      "source": [
        "#Extract the tar file as dataset\n",
        "\n",
        "import tarfile\n",
        "\n",
        "def tarfile_extract(tar_file, output_dir=os.getcwd()+'/Datasets'):\n",
        "    tar = tarfile.open(tar_file, 'r:gz')\n",
        "    total_files = sum(1 for _ in tar)\n",
        "    tar.extractall(output_dir, members=extract_progress(tar, total_files))\n",
        "    tar.close()\n",
        "\n",
        "def extract_progress(tar, total_files):\n",
        "    for member in tar:\n",
        "        yield member\n",
        "        total_files -= 1\n",
        "        print(f\"Remaining files: {total_files}\", end='\\r')\n",
        "    print(\"\\nExtraction completed.\")\n",
        "\n",
        "wd = os.getcwd()\n",
        "tarfile_extract(wd+'/Datasets/aclImdb_v1.tar.gz')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7ezNM9_W3jQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#using 'alldata' list to store all the files in the directories\n",
        "alldata=[]\n",
        "\n",
        "\n",
        "#collecting data in train/pos folder\n",
        "for fname in os.listdir(wd+'/Datasets/aclImdb/train/pos'):\n",
        "    with open(os.path.join(wd+'/Datasets/aclImdb/train/pos', fname), encoding = 'utf-8') as infile:\n",
        "        for line in infile:\n",
        "            alldata.append((line,'train','pos'))\n",
        "\n",
        "#collecting data in train/neg folder\n",
        "for fname in os.listdir(wd+'/Datasets/aclImdb/train/neg'):\n",
        "    with open(os.path.join(wd+'/Datasets/aclImdb/train/neg', fname), encoding = 'utf-8') as infile:\n",
        "        for line in infile:\n",
        "            alldata.append((line,'train','neg'))\n",
        "#collecting data in test/pos folder\n",
        "for fname in os.listdir(wd+'/Datasets/aclImdb/test/pos'):\n",
        "    with open(os.path.join(wd+'/Datasets/aclImdb/test/pos', fname), encoding = 'utf-8') as infile:\n",
        "        for line in infile:\n",
        "            alldata.append((line,'test','pos'))\n",
        "#collecting data in test/neg folder\n",
        "for fname in os.listdir(wd+'/Datasets/aclImdb/test/neg'):\n",
        "    with open(os.path.join(wd+'/Datasets/aclImdb/test/neg', fname), encoding = 'utf-8') as infile:\n",
        "        for line in infile:\n",
        "            alldata.append((line,'test','neg'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uH1TGbjlh7o",
        "outputId": "c43c9ce1-1a78-41f4-8034-1c3e5b3c537f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
        "\n",
        "appName = \"list to Spark Data Frame\"\n",
        "master = \"local\"\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(appName) \\\n",
        "    .master(master) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# List\n",
        "data = alldata\n",
        "\n",
        "# Create a schema for the dataframe\n",
        "schema = StructType([\n",
        "    StructField('content', StringType(), True),\n",
        "    StructField('label', StringType(), True),\n",
        "    StructField('sentiemtn', StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert list to RDD\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Create data frame\n",
        "df = spark.createDataFrame(rdd,schema)\n",
        "print(df.schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7DUME6Xx1Hx",
        "outputId": "fb816dd9-aaf3-4af3-b678-577c5bbfa3b8"
      },
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JoFIyowyMjB",
        "outputId": "ff0c2ad5-97bf-4773-c4b8-9bffe15c187c"
      },
      "outputs": [],
      "source": [
        "# Schema of created Spark Dataframe\n",
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPXsyhn_2QZC"
      },
      "source": [
        "###Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK1PwZq5yhfK",
        "outputId": "e8f45810-67b3-4db0-bfc9-0c8656d7ca26"
      },
      "outputs": [],
      "source": [
        "#using RegexTokenizer for tokenizing contents\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"content\", outputCol=\"tokenized_content\", pattern=\"\\\\W\") # used regexp to determine pattern as 'not word'\n",
        "\n",
        "countTokens = udf(lambda w: len(w), IntegerType())\n",
        "\n",
        "tokenized = tokenizer.transform(df)\n",
        "\n",
        "tokenized.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf-Omq8n15FM"
      },
      "source": [
        "### Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NcWM3ek-iph",
        "outputId": "ead4cd37-4991-4f5d-ee5a-3c658913581c"
      },
      "outputs": [],
      "source": [
        "type(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtoV10AW1uN6",
        "outputId": "36fc2c88-3b56-44d6-e93f-99565adab26e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "tokenized.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN6Qe86F4cPF"
      },
      "outputs": [],
      "source": [
        "df_tokenized = tokenized.select(\"tokenized_content\").withColumn(\"tokenCount\", countTokens(col(\"tokenized_content\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6gu2PKO9bkw",
        "outputId": "21a74ae9-84cd-494c-f73d-67eaeac6a24a"
      },
      "outputs": [],
      "source": [
        "type(df_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1JaxUOh3eIL",
        "outputId": "906b1c9e-4510-43cf-8b02-dd2d89ecbb6a"
      },
      "outputs": [],
      "source": [
        "# SWR -> stop word remover\n",
        "SWR  = StopWordsRemover (inputCol='tokenized_content', outputCol='SWRed')\n",
        "\n",
        "\n",
        "#See the result of removal operation\n",
        "SWR.transform(df_tokenized).select('SWRed').show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Huawei R&D Technical Interview Question_v2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
