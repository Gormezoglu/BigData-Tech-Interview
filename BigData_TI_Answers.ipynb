{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ybl-OTpuiQx"
      },
      "source": [
        "## Task -1 Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2FGpclC3tn4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Installing Spark with its dependencies\n",
        "Installing Spark\n",
        "Install Dependencies:\n",
        "\n",
        "Java 8\n",
        "Apache Spark with hadoop and\n",
        "Findspark (used to locate the spark in the system)\n",
        "\"\"\"\n",
        "\n",
        "!sudo ./install_spark.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vd9yVaK4DR6"
      },
      "outputs": [],
      "source": [
        "#Set Environment Variables:\n",
        "\n",
        "import os\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = os.path.join(current_directory,\"spark-3.1.1-bin-hadoop3.2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "SUjv6TWh4i1t",
        "outputId": "9bb2d972-e87e-43fb-d2f5-fca2dd7c7b1b"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mYOKttpn9EH"
      },
      "outputs": [],
      "source": [
        "#will be used sqlite3 to be able to reach .db file\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "con = sqlite3.connect('Datasets/movielens-small.db')\n",
        "cur = con.cursor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OahMiKvLqz6p",
        "outputId": "9b7a9f5d-808f-409d-e6fa-8b5d122c5de0"
      },
      "outputs": [],
      "source": [
        "#For SQLite JDBC driver, it can be downloaded via:\n",
        "\n",
        "!curl -O https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "9K03GzCfx8jU",
        "outputId": "cca7859b-a5f0-406a-81df-c3f86529f54a"
      },
      "outputs": [],
      "source": [
        "# Write a SQL query to create a dataframe with including userid, movieid, genre and rating\n",
        "\n",
        "import csv\n",
        "with open(\"movielens.csv\", \"w\") as csvFile:\n",
        "    fieldnames = ['userId', 'movieId', 'genre', 'rating']\n",
        "    writer = csv.DictWriter(csvFile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for row in cur.execute(\"\"\"SELECT ratings.userId, movies.movieId, genres, rating \n",
        "                              FROM (((movies \n",
        "                                      INNER JOIN links ON movies.movieid = links.movieid) \n",
        "                                      LEFT JOIN ratings ON movies.movieId = ratings.movieId) \n",
        "                                      LEFT JOIN tags ON movies.movieid = tags.movieid)\"\"\"):\n",
        " \n",
        "        userId = row[0]\n",
        "        movieId = row[1]\n",
        "        genre = row[2]\n",
        "        rating = row[3]\n",
        "\n",
        "        writer.writerow({'userId': userId, \n",
        "                        'movieId':movieId,\n",
        "                        'genre': genre,\n",
        "                        'rating':rating}\n",
        "        )\n",
        "\n",
        "movielens_small_df = spark.read.csv(\"movielens.csv\", header=True)\n",
        "\n",
        "print(\"number of rows of our dataframe:\", movielens_small_df.count())\n",
        "print(\"\\n\")\n",
        "\n",
        "movielens_small_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9adqNcucRA00",
        "outputId": "c08790f9-8ab1-4de5-8c3f-958c4d1c7365"
      },
      "outputs": [],
      "source": [
        "# Count ratings for each movie, and list top 5 movies with the highest value\n",
        "\n",
        "movie_rating_count = []\n",
        "\n",
        "for row in cur.execute(\"\"\"SELECT movieid, title,COUNT(rating) \n",
        "                          FROM (SELECT ratings.userId, movies.movieId, title,genres, rating \n",
        "                                FROM (((movies \n",
        "                                        INNER JOIN links ON movies.movieid = links.movieid)\n",
        "                                        LEFT JOIN ratings ON movies.movieId = ratings.movieId)\n",
        "                                        LEFT JOIN tags ON movies.movieid = tags.movieid))                                                    \n",
        "                          GROUP BY movieid\n",
        "                          ORDER BY count(rating) DESC\n",
        "                          LIMIT 5\n",
        "                        \"\"\"):\n",
        "  movie_rating_count.append(row)\n",
        "\n",
        "schema = [\"movieId\", \"title\", \"rating count\"]\n",
        " \n",
        "# calling function to create dataframe\n",
        "df = spark.createDataFrame(movie_rating_count, schema)\n",
        "\n",
        "df.show(truncate=False)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgVd_W6qkFC_",
        "outputId": "7dccc28f-4a43-4244-dc05-5c1aacdb347d"
      },
      "outputs": [],
      "source": [
        "# Find and list top 5 most rated genres\n",
        "\n",
        "rated_genres = []\n",
        "\n",
        "for row in cur.execute(\"\"\"SELECT genres, COUNT(rating) \n",
        "                          FROM (SELECT ratings.userId, movies.movieId, genres, rating \n",
        "                                FROM (((movies \n",
        "                                        INNER JOIN links ON movies.movieid = links.movieid) \n",
        "                                        LEFT JOIN ratings ON movies.movieId = ratings.movieId) \n",
        "                                        LEFT JOIN tags ON movies.movieid = tags.movieid))\n",
        "                          GROUP BY genres\n",
        "                          ORDER BY COUNT(rating) DESC\n",
        "                          LIMIT 5\n",
        "                        \"\"\"):\n",
        "  rated_genres.append(row)\n",
        "\n",
        "schema = [\"genres\", \"rating count\"]\n",
        " \n",
        "# calling function to create dataframe\n",
        "df = spark.createDataFrame(rated_genres, schema)\n",
        "\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4OzuDxlcDsL",
        "outputId": "ef97a8b8-5517-495c-a035-56c6c1562f5f"
      },
      "outputs": [],
      "source": [
        "#Find and list top 5 most rated tags\n",
        "\n",
        "rated_tags = []\n",
        "\n",
        "for row in cur.execute(\"\"\"SELECT tag, COUNT(rating) \n",
        "                          FROM (SELECT ratings.userId, movies.movieId, genres, tag, rating \n",
        "                                FROM (((movies \n",
        "                                        INNER JOIN links ON movies.movieid = links.movieid) \n",
        "                                        LEFT JOIN ratings ON movies.movieId = ratings.movieId) \n",
        "                                        LEFT JOIN tags ON movies.movieid = tags.movieid))\n",
        "                          WHERE tag is NOT NULL\n",
        "                          GROUP BY tag\n",
        "                          ORDER BY COUNT(rating) DESC\n",
        "                          LIMIT 5\n",
        "                        \"\"\"):\n",
        "  rated_tags.append(row)\n",
        "\n",
        "schema = [\"tags\", \"tags count\"]\n",
        " \n",
        "# calling function to create dataframe\n",
        "df = spark.createDataFrame(rated_tags, schema)\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zWDyIsOk-uz",
        "outputId": "d4ad6c27-10c1-4f19-ad5e-e837a26c8012"
      },
      "outputs": [],
      "source": [
        "# By using timestamp from ratings table, provide top 5 most frequent users within a week\n",
        "\n",
        "weekly_activity = []\n",
        "\n",
        "for row in cur.execute(\"\"\"SELECT userid, strftime('%Y-%W', datetime(timestamp, 'unixepoch')) AS week_year, COUNT(strftime('%Y-%W', datetime(timestamp, 'unixepoch'))) AS weekly_activity \n",
        "                          FROM ratings\n",
        "                          GROUP BY userid, week_year \n",
        "                          ORDER BY weekly_activity DESC\n",
        "                          LIMIT 5\n",
        "                        \"\"\"):\n",
        "  weekly_activity.append(row)\n",
        "\n",
        "schema = [\"userId\", \"week of the year\", \"weekly activity of user\"]\n",
        " \n",
        "# calling function to create dataframe\n",
        "df = spark.createDataFrame(weekly_activity, schema)\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK3PElZAxM7a",
        "outputId": "30ae116e-0681-41a4-84d1-f1d14be82561"
      },
      "outputs": [],
      "source": [
        "# Calculate average ratings for each genre, and plot average ratings of top 10 genres with descending order\n",
        "\n",
        "genres_avgrating=[]\n",
        "\n",
        "for row in cur.execute(\"\"\"SELECT genres, round(avg(rating), 2) avg_rating \n",
        "                          FROM (SELECT ratings.userId, movies.movieId, genres, rating \n",
        "                                FROM (((movies \n",
        "                                        INNER JOIN links ON movies.movieid = links.movieid) \n",
        "                                        LEFT JOIN ratings ON movies.movieId = ratings.movieId) \n",
        "                                        LEFT JOIN tags ON movies.movieid = tags.movieid))\n",
        "                          GROUP BY genres\n",
        "                          ORDER BY avg_rating DESC\n",
        "                          LIMIT 10\"\"\"):\n",
        "  genres_avgrating.append(row)\n",
        "\n",
        "schema = [\"genres\", \"avg_rating\"]\n",
        " \n",
        "# calling function to create dataframe\n",
        "df = spark.createDataFrame(genres_avgrating, schema)\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M2z4B1ZubTP"
      },
      "source": [
        "## TASK 2 - Recommender Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd6rfJRvuf0U"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0YoCcJzPkrB"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"movieRecommendation\").getOrCreate() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "XlqTJcjFSdHr",
        "outputId": "4f9bf3d2-7be9-42f8-af34-eb03fc282028"
      },
      "outputs": [],
      "source": [
        "# using movie rating data to probide implicit feature using ALS(Alternate Least Squares)\n",
        "\n",
        "movie_rating=[]\n",
        "\n",
        "for row in cur.execute(\"\"\"SELECT userId, movieId, rating \n",
        "                          FROM ratings\n",
        "                                      \"\"\"):\n",
        "  movie_rating.append(row)\n",
        "\n",
        "schema = [\"userId\", \"movieId\", \"rating\"]\n",
        " \n",
        "# calling function to create dataframe\n",
        "movie_rating_df = spark.createDataFrame(movie_rating, schema)\n",
        "\n",
        "movie_rating_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sHsJtxjKL9A",
        "outputId": "0d4470c6-577b-47d7-94be-fb36f80937d6"
      },
      "outputs": [],
      "source": [
        "# description of created schema \n",
        "movie_rating_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0xQG5w9ryG8"
      },
      "outputs": [],
      "source": [
        "#splitting dataset to train the model as 80% for train and remaining for test data.\n",
        "(train, test) = movie_rating_df.randomSplit([0.8, 0.2], seed=87)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGTNsGakWNl6",
        "outputId": "09fa8371-5c7b-425b-de3b-0cb88923645b"
      },
      "outputs": [],
      "source": [
        "# 1st Recommender model - Alternating Least Square (ALS) Matrix Factorization in Collaborative Filtering on rating (as actual values) \n",
        "\n",
        "als = ALS(rank=10, maxIter=15, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
        "\n",
        "model = als.fit(train)\n",
        "\n",
        "pred = model.transform(test)\n",
        "\n",
        "pred = pred.selectExpr(\"userId as userId\",\"movieId as movieId\",\"rating as rating\",\"prediction as implicit\")\n",
        "\n",
        "pred.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IxWorgqcUIO",
        "outputId": "bf56b94f-7c1b-4cb9-fc51-04d2eb244d95"
      },
      "outputs": [],
      "source": [
        "#calculating RMSE and MAE to evaluate performance of the models. \n",
        "\n",
        "eval_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"implicit\")\n",
        "eval_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"implicit\")\n",
        "\n",
        "\n",
        "rmse = eval_rmse.evaluate(pred)\n",
        "mae = eval_mae.evaluate(pred)\n",
        "\n",
        "\n",
        "print(\"RMSE of ALS:\", rmse)\n",
        "print(\"MAE of ALS:\", mae)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiveQdC4vbT-",
        "outputId": "792174a8-f360-48ab-9e95-b73b6c8f1410"
      },
      "outputs": [],
      "source": [
        "# 2nd Recommender model - Alternating Least Square (ALS) Matrix Factorization in Collaborative Filtering on designed implicit feedback values \n",
        "\n",
        "\n",
        "(train_implicit, test_implicit) = pred.randomSplit([0.8, 0.2], seed=87)\n",
        "\n",
        "als_implicit = ALS(rank=10, maxIter=15, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"implicit\", coldStartStrategy=\"drop\")\n",
        "\n",
        "model_implicit = als_implicit.fit(train_implicit)\n",
        "\n",
        "pred_implicit = model_implicit.transform(test_implicit)\n",
        "\n",
        "pred_implicit.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oUlsznN7MGh",
        "outputId": "b1757443-91a5-44e8-8f03-32982684794f"
      },
      "outputs": [],
      "source": [
        "eval_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "eval_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "\n",
        "\n",
        "rmse = eval_rmse.evaluate(pred_implicit)\n",
        "mae = eval_mae.evaluate(pred_implicit)\n",
        "\n",
        "\n",
        "print(\"RMSE of ALS_Implicit Feedback:\", rmse)\n",
        "print(\"MAE of ALS_Implicit Feedback:\", mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imQMCTu799AA"
      },
      "source": [
        "When compared these two models, the 1st model ( ALS on rating ) shows better performance compared to 2nd model (ALS on implicit feedback) according to error metrics such as Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeRDX3e3-4z5"
      },
      "source": [
        "## Task â€“ 3 Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY-LEz8E-8CP"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"textAnalysis\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXVe2XVGHOut"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download the data in Dataset folder.\n",
        "\n",
        "\n",
        "\n",
        "!curl -o Datasets/aclImdb_v1.tar.gz https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_13BV8pDCDiN",
        "outputId": "d7ac9527-f4af-4524-824a-f5a679f38650"
      },
      "outputs": [],
      "source": [
        "#Extract the tar file as dataset\n",
        "\n",
        "import tarfile\n",
        "\n",
        "def tarfile_extract(tar_file, output_dir=os.getcwd()+'/Datasets'):\n",
        "    tar = tarfile.open(tar_file, 'r:gz')\n",
        "    total_files = sum(1 for _ in tar)\n",
        "    tar.extractall(output_dir, members=extract_progress(tar, total_files))\n",
        "    tar.close()\n",
        "\n",
        "def extract_progress(tar, total_files):\n",
        "    for member in tar:\n",
        "        yield member\n",
        "        total_files -= 1\n",
        "        print(f\"Remaining files: {total_files}\", end='\\r')\n",
        "    print(\"\\nExtraction completed.\")\n",
        "\n",
        "wd = os.getcwd()\n",
        "tarfile_extract(wd+'/Datasets/aclImdb_v1.tar.gz')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7ezNM9_W3jQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#using 'alldata' list to store all the files in the directories\n",
        "alldata=[]\n",
        "\n",
        "\n",
        "#collecting data in train/pos folder\n",
        "for fname in os.listdir(wd+'/Datasets/aclImdb/train/pos'):\n",
        "    with open(os.path.join(wd+'/Datasets/aclImdb/train/pos', fname), encoding = 'utf-8') as infile:\n",
        "        for line in infile:\n",
        "            alldata.append((line,'train','pos'))\n",
        "\n",
        "#collecting data in train/neg folder\n",
        "for fname in os.listdir(wd+'/Datasets/aclImdb/train/neg'):\n",
        "    with open(os.path.join(wd+'/Datasets/aclImdb/train/neg', fname), encoding = 'utf-8') as infile:\n",
        "        for line in infile:\n",
        "            alldata.append((line,'train','neg'))\n",
        "#collecting data in test/pos folder\n",
        "for fname in os.listdir(wd+'/Datasets/aclImdb/test/pos'):\n",
        "    with open(os.path.join(wd+'/Datasets/aclImdb/test/pos', fname), encoding = 'utf-8') as infile:\n",
        "        for line in infile:\n",
        "            alldata.append((line,'test','pos'))\n",
        "#collecting data in test/neg folder\n",
        "for fname in os.listdir(wd+'/Datasets/aclImdb/test/neg'):\n",
        "    with open(os.path.join(wd+'/Datasets/aclImdb/test/neg', fname), encoding = 'utf-8') as infile:\n",
        "        for line in infile:\n",
        "            alldata.append((line,'test','neg'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uH1TGbjlh7o",
        "outputId": "c43c9ce1-1a78-41f4-8034-1c3e5b3c537f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
        "\n",
        "appName = \"list to Spark Data Frame\"\n",
        "master = \"local\"\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(appName) \\\n",
        "    .master(master) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# List\n",
        "data = alldata\n",
        "\n",
        "# Create a schema for the dataframe\n",
        "schema = StructType([\n",
        "    StructField('content', StringType(), True),\n",
        "    StructField('label', StringType(), True),\n",
        "    StructField('sentiemtn', StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert list to RDD\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Create data frame\n",
        "df = spark.createDataFrame(rdd,schema)\n",
        "print(df.schema)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7DUME6Xx1Hx",
        "outputId": "fb816dd9-aaf3-4af3-b678-577c5bbfa3b8"
      },
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JoFIyowyMjB",
        "outputId": "ff0c2ad5-97bf-4773-c4b8-9bffe15c187c"
      },
      "outputs": [],
      "source": [
        "# Schema of created Spark Dataframe\n",
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPXsyhn_2QZC"
      },
      "source": [
        "###Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK1PwZq5yhfK",
        "outputId": "e8f45810-67b3-4db0-bfc9-0c8656d7ca26"
      },
      "outputs": [],
      "source": [
        "#using RegexTokenizer for tokenizing contents\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"content\", outputCol=\"tokenized_content\", pattern=\"\\\\W\") # used regexp to determine pattern as 'not word'\n",
        "\n",
        "countTokens = udf(lambda w: len(w), IntegerType())\n",
        "\n",
        "tokenized = tokenizer.transform(df)\n",
        "\n",
        "tokenized.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf-Omq8n15FM"
      },
      "source": [
        "### Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NcWM3ek-iph",
        "outputId": "ead4cd37-4991-4f5d-ee5a-3c658913581c"
      },
      "outputs": [],
      "source": [
        "type(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtoV10AW1uN6",
        "outputId": "36fc2c88-3b56-44d6-e93f-99565adab26e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "tokenized.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN6Qe86F4cPF"
      },
      "outputs": [],
      "source": [
        "df_tokenized = tokenized.select(\"tokenized_content\").withColumn(\"tokenCount\", countTokens(col(\"tokenized_content\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6gu2PKO9bkw",
        "outputId": "21a74ae9-84cd-494c-f73d-67eaeac6a24a"
      },
      "outputs": [],
      "source": [
        "type(df_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1JaxUOh3eIL",
        "outputId": "906b1c9e-4510-43cf-8b02-dd2d89ecbb6a"
      },
      "outputs": [],
      "source": [
        "# SWR -> stop word remover\n",
        "SWR  = StopWordsRemover (inputCol='tokenized_content', outputCol='SWRed')\n",
        "\n",
        "\n",
        "#See the result of removal operation\n",
        "SWR.transform(df_tokenized).select('SWRed').show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Huawei R&D Technical Interview Question_v2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
